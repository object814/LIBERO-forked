{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This jupyter notebook covers the following contents:\n",
    "1. Default configuration in LIBERO\n",
    "2. Basic information about available LIBERO benchmarks\n",
    "   - Get a dictionary of mapping from benchmark name to benchmark class\n",
    "   - Check the integrity of benchmarks\n",
    "   - Check the integrity of init files\n",
    "   - Visualize all the init states of a task\n",
    "   - Download datasets\n",
    "   - Get information about a demonstration file and replay a trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libero.libero import benchmark, get_libero_path, set_libero_default_path\n",
    "import os\n",
    "from termcolor import colored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Default file paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the paths are retrieved from a yaml config file located at `~/.libero/config.yaml`. And the default paths are set to relative to the libero codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_root_path = get_libero_path(\"benchmark_root\")\n",
    "init_states_default_path = get_libero_path(\"init_states\")\n",
    "datasets_default_path = get_libero_path(\"datasets\")\n",
    "bddl_files_default_path = get_libero_path(\"bddl_files\")\n",
    "print(\"Default benchmark root path: \", benchmark_root_path)\n",
    "print(\"Default dataset root path: \", datasets_default_path)\n",
    "print(\"Default bddl files root path: \", bddl_files_default_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you want to point your codebase to custom path, you can use `set_libero_path` function to do that. Notice that all the paths change according to `benchmark_root` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_libero_default_path(os.path.join(os.path.expanduser(\"~\"), \"custom_project\"))\n",
    "benchmark_root_path = get_libero_path(\"benchmark_root\")\n",
    "init_states_default_path = get_libero_path(\"init_states\")\n",
    "datasets_default_path = get_libero_path(\"datasets\")\n",
    "bddl_files_default_path = get_libero_path(\"bddl_files\")\n",
    "print(\"Default benchmark root path: \", benchmark_root_path)\n",
    "print(\"Default dataset root path: \", datasets_default_path)\n",
    "print(\"Default bddl files root path: \", bddl_files_default_path)\n",
    "\n",
    "# If nothing is specified in the `set_libero_default_path` function, the path will be changed back to the default path\n",
    "# We will set back the path to the default path for the subsequent examples\n",
    "set_libero_default_path()\n",
    "benchmark_root_path = get_libero_path(\"benchmark_root\")\n",
    "init_states_default_path = get_libero_path(\"init_states\")\n",
    "datasets_default_path = get_libero_path(\"datasets\")\n",
    "bddl_files_default_path = get_libero_path(\"bddl_files\")\n",
    "print(\"Default benchmark root path: \", benchmark_root_path)\n",
    "print(\"Default dataset root path: \", datasets_default_path)\n",
    "print(\"Default bddl files root path: \", bddl_files_default_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. See available benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get a dictionary of mapping from benchmark name to benchmark class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "print(benchmark_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check the integrity of benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a benchmark\n",
    "benchmark_instance = benchmark_dict[\"libero_10\"]()\n",
    "num_tasks = benchmark_instance.get_num_tasks()\n",
    "# see how many tasks involved in the benchmark\n",
    "print(f\"{num_tasks} tasks in the benchmark {benchmark_instance.name}: \")\n",
    "\n",
    "# Check if all the task names and their bddl file names\n",
    "task_names = benchmark_instance.get_task_names()\n",
    "print(\"The benchmark contains the following tasks:\")\n",
    "for i in range(num_tasks):\n",
    "    task_name = task_names[i]\n",
    "    task = benchmark_instance.get_task(i)\n",
    "    bddl_file = os.path.join(bddl_files_default_path, task.problem_folder, task.bddl_file)\n",
    "    print(f\"\\t {task_name}, detail definition stored in {bddl_file}\")\n",
    "    if not os.path.exists(bddl_file):\n",
    "        print(colored(f\"[error] bddl file {bddl_file} cannot be found. Check your paths\", \"red\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Check the integrity of init files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the init states files exist for tasks\n",
    "task_names = benchmark_instance.get_task_names()\n",
    "print(\"The benchmark contains the following tasks:\")\n",
    "for i in range(num_tasks):\n",
    "    task_name = task_names[i]\n",
    "    task = benchmark_instance.get_task(i)\n",
    "    init_states_path = os.path.join(init_states_default_path, task.problem_folder, task.init_states_file)\n",
    "    if not os.path.exists(init_states_path):\n",
    "        print(colored(f\"[error] the init states {init_states_path} cannot be found. Check your paths\", \"red\"))\n",
    "print(f\"An example of init file is named like this: {task.init_states_file}\")\n",
    "\n",
    "# Load torch init files\n",
    "init_states = benchmark_instance.get_task_init_states(0)\n",
    "# Init states in the same (num_init_rollouts, num_simulation_states)\n",
    "print(init_states.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize all the init states of a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# task_id is the (task_id + 1)th task in the benchmark\n",
    "task_id = 9\n",
    "task = benchmark_instance.get_task(task_id)\n",
    "\n",
    "env_args = {\n",
    "    \"bddl_file_name\": os.path.join(bddl_files_default_path, task.problem_folder, task.bddl_file),\n",
    "    \"camera_heights\": 128,\n",
    "    \"camera_widths\": 128\n",
    "}\n",
    "\n",
    "env = OffScreenRenderEnv(**env_args)\n",
    "\n",
    "\n",
    "init_states = benchmark_instance.get_task_init_states(task_id)\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "env.seed(0)\n",
    "\n",
    "def make_grid(images, nrow=8, padding=2, normalize=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images. Make sure images is a 4D tensor in the shape of (B x C x H x W)) or a list of torch tensors.\"\"\"\n",
    "    grid_image = torchvision.utils.make_grid(images, nrow=nrow, padding=padding, normalize=normalize, pad_value=pad_value).permute(1, 2, 0)\n",
    "    return grid_image\n",
    "\n",
    "images = []\n",
    "env.reset()\n",
    "for eval_index in range(len(init_states)):\n",
    "    env.set_init_state(init_states[eval_index])\n",
    "\n",
    "    for _ in range(5):\n",
    "        obs, _, _, _ = env.step([0.] * 7)\n",
    "    images.append(torch.from_numpy(obs[\"agentview_image\"]).permute(2, 0, 1))\n",
    "\n",
    "# # images = torch.stack(images, dim=0).permute(0, 3, 1, 2)\n",
    "# print(images.shape)\n",
    "grid_image = make_grid(images, nrow=10, padding=2, pad_value=0)\n",
    "display(Image.fromarray(grid_image.numpy()[::-1]))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libero.libero.utils.download_utils as download_utils\n",
    "\n",
    "download_dir = get_libero_path(\"datasets\")\n",
    "datasets = \"libero_spatial\" # Can specify \"all\", \"libero_goal\", \"libero_spatial\", \"libero_object\", \"libero_100\"\n",
    "\n",
    "libero_datasets_exist = download_utils.check_libero_dataset(download_dir=download_dir)\n",
    "\n",
    "if not libero_datasets_exist:\n",
    "    download_utils.libero_dataset_download(download_dir=download_dir, datasets=datasets)\n",
    "\n",
    "# Check if the demo files exist\n",
    "demo_files = [os.path.join(datasets_default_path, benchmark_instance.get_task_demonstration(i)) for i in range(num_tasks)]\n",
    "for demo_file in demo_files:\n",
    "    if not os.path.exists(demo_file):\n",
    "        print(colored(f\"[error] demo file {demo_file} cannot be found. Check your paths\", \"red\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Get information about a demonstration file and replay a trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from libero.libero.utils.dataset_utils import get_dataset_info\n",
    "from IPython.display import HTML\n",
    "import imageio\n",
    "\n",
    "example_demo_file = demo_files[9]\n",
    "# Print the dataset info. We have a standalone script for doing the same thing available at `scripts/get_dataset_info.py`\n",
    "get_dataset_info(example_demo_file)\n",
    "\n",
    "with h5py.File(example_demo_file, \"r\") as f:\n",
    "    images = f[\"data/demo_0/obs/agentview_rgb\"][()]\n",
    "\n",
    "video_writer = imageio.get_writer(\"output.mp4\", fps=60)\n",
    "for image in images:\n",
    "    video_writer.append_data(image[::-1])\n",
    "video_writer.close()\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"output.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <script>\n",
    "        var video = document.getElementsByTagName('video')[0];\n",
    "        video.playbackRate = 2.0; // Increase the playback speed to 2x\n",
    "        </script>    \n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Concate multiple datasets for multit-task training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "from libero.lifelong.datasets import get_dataset, SequenceVLDataset\n",
    "\n",
    "num_tasks = benchmark_instance.get_num_tasks()\n",
    "print(f\"Number of tasks in the benchmark {benchmark_instance.name}: {num_tasks}\")\n",
    "\n",
    "# manip_datasets = []\n",
    "# for demo_file in demo_files:\n",
    "#     task_i_dataset, shape_meta = get_dataset(\n",
    "#             dataset_path=os.path.join(cfg.folder,\n",
    "#                                         benchmark.get_task_demonstration(i)),\n",
    "#             obs_modality=cfg.data.obs.modality,\n",
    "#             initialize_obs_utils=(i==0),\n",
    "#             seq_len=cfg.data.seq_len)    \n",
    "#     manip_datasets.append()\n",
    "\n",
    "# concat_dataset = ConcatDataset([get_dataset(demo_file) for demo_file in demo_files])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Create datasets for Experience Replay algorithm\n",
    "\n",
    "In the algorithm of ER, we need to sample data from both dataset of the current task and data from previous experiences. To this end, a specific implementation is needed (`TruncatedSequenceDataset`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64fd624917382b0c0ee6e40067ed4768d5d5501e9a56437104405cabbecfa898"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('libero': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
